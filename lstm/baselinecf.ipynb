{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e7b85dc",
   "metadata": {},
   "source": [
    "ขั้นตอนนี้คือโหลด dataset จาก .pt เอามาดู x, y shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30800005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_all shape: torch.Size([173, 32, 53])\n",
      "y_all shape: torch.Size([173])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "MODEL_PATH = \"/Users/sarawit/Documents/Year2/sem2/Artificial-inteligent/FINAL_PROJECT/fnpj/csi_dataset/csi_windows_w64_s32.pt\"\n",
    "\n",
    "data = torch.load(MODEL_PATH, map_location=\"cpu\")\n",
    "\n",
    "X_all = data[\"X\"]   # expected shape: (N, 64, F)\n",
    "y_all = data[\"y\"]   # expected shape: (N,)\n",
    "\n",
    "print(\"X_all shape:\", X_all.shape)\n",
    "print(\"y_all shape:\", y_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0468fedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes: [0 1 2]\n",
      "counts : [21 79 73]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "unique, counts = np.unique(y_all, return_counts=True)\n",
    "print(\"classes:\", unique)\n",
    "print(\"counts :\", counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef23555",
   "metadata": {},
   "source": [
    "This code performs cross-validation-like evaluation by training and testing an LSTM model multiple times with different random seeds to assess performance stability. It iterates over a list of seeds, splits the data into train/test sets using stratified sampling, creates new datasets and data loaders each time, initializes a fresh model, trains it for 10 epochs, and then evaluates accuracy on the test set. This helps check if the model's performance varies significantly with different data splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9615164f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Random seed: 0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'CSIDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     11\u001b[39m X_train, X_test, y_train, y_test = train_test_split(\n\u001b[32m     12\u001b[39m     X_all, y_all,\n\u001b[32m     13\u001b[39m     test_size=\u001b[32m0.2\u001b[39m,\n\u001b[32m     14\u001b[39m     random_state=seed,\n\u001b[32m     15\u001b[39m     stratify=y_all\n\u001b[32m     16\u001b[39m )\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# 2️⃣ สร้าง dataset + dataloader ใหม่\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m train_ds = \u001b[43mCSIDataset\u001b[49m(X_train, y_train)\n\u001b[32m     20\u001b[39m test_ds  = CSIDataset(X_test, y_test)\n\u001b[32m     22\u001b[39m train_loader = DataLoader(train_ds, batch_size=\u001b[32m32\u001b[39m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'CSIDataset' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "seeds = [0, 1, 2, 3, 4, 5, 10, 42, 99]\n",
    "\n",
    "for seed in seeds:\n",
    "    print(\"\\n==============================\")\n",
    "    print(\"Random seed:\", seed)\n",
    "\n",
    "    # 1️⃣ split ใหม่ทุกครั้ง\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_all, y_all,\n",
    "        test_size=0.2,\n",
    "        random_state=seed,\n",
    "        stratify=y_all\n",
    "    )\n",
    "\n",
    "    # 2️⃣ สร้าง dataset + dataloader ใหม่\n",
    "    train_ds = CSIDataset(X_train, y_train)\n",
    "    test_ds  = CSIDataset(X_test, y_test)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=32, shuffle=False)\n",
    "\n",
    "    # 3️⃣ สร้าง model ใหม่ทุกครั้ง (สำคัญ!)\n",
    "    model = LSTMClassifier(input_size=53, hidden_size=128, num_layers=2, num_classes=3)\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # 4️⃣ train แค่ 10 epoch พอ\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            yb = yb.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # 5️⃣ evaluate test\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            logits = model(xb)\n",
    "            preds = logits.argmax(dim=1).cpu()\n",
    "            correct += (preds == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "\n",
    "    acc = correct / total\n",
    "    print(\"Test accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2393a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# เลือก device (ถ้ามี MPS บน Mac ก็ใช้)\n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "class CSIDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        # X, y ตอนนี้เป็น torch.Tensor อยู่แล้วจาก train_test_split\n",
    "        # ถ้าไม่ใช่ tensor ให้ครอบด้วย torch.tensor(...)\n",
    "        self.X = X.float()\n",
    "        self.y = y.long()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_ds = CSIDataset(X_train, y_train)\n",
    "test_ds  = CSIDataset(X_test,  y_test)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(\"Train batches:\", len(train_loader))\n",
    "print(\"Test  batches:\", len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8c5a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=2, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True   # ทำให้ shape เป็น (batch, seq, feature)\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, F) = (batch_size, sequence_length, num_features)\n",
    "        out, _ = self.lstm(x)        # out: (B, T, H)\n",
    "        last = out[:, -1, :]         # เอา timestep สุดท้าย: (B, H)\n",
    "        logits = self.fc(last)       # (B, num_classes)\n",
    "        return logits\n",
    "\n",
    "input_size = X_all.shape[2]  # = 53\n",
    "model = LSTMClassifier(input_size=input_size, hidden_size=128, num_layers=2, num_classes=3)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()            # baseline ยังไม่ใส่ class weight\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869759da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # ===== TRAIN =====\n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        yb = yb.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * yb.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total += yb.size(0)\n",
    "\n",
    "    epoch_acc = correct / total\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | train_acc={epoch_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3cbe6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        logits = model(xb)\n",
    "        preds = logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "        all_preds.extend(preds)\n",
    "        all_true.extend(yb.numpy())\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "all_true = np.array(all_true)\n",
    "\n",
    "cm = confusion_matrix(all_true, all_preds, labels=[0,1,2])\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(all_true, all_preds, digits=3, target_names=[\"no_human\",\"static\",\"movement\"]))\n",
    "\n",
    "# plot confusion matrix ให้ดูเหมือนของเพื่อน\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.imshow(cm, cmap=\"viridis\")\n",
    "plt.title(\"Confusion Matrix (Test Set)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.colorbar()\n",
    "\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, str(cm[i, j]), ha=\"center\", va=\"center\", color=\"white\")\n",
    "\n",
    "plt.xticks([0,1,2], [0,1,2])\n",
    "plt.yticks([0,1,2], [0,1,2])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
